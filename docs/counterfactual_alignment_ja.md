# Redefining AGI Alignment via Counterfactual Reasoning（反事実推論によるアライメントの再定義）

## 1. 要約（Abstract）

本ドキュメントは、現在の AGI アライメントにおける根本的な欠陥として  
**「反事実推論の欠如」** に焦点を当てます。

多くのアライメント問題は、現代の AI システムが次のことを十分に行えないことに由来すると考えます。

- 明示的な **反事実状態（counterfactual state）** を生成・維持すること
- その仮定状態の下で行動をシミュレートし、評価すること
- 現実と仮定の両方のシナリオにおいて、一貫した倫理性を保つこと

ここでは、既存の LLM 系システムや軽量テンプレート推論エンジンに  
段階的に組み込める形での、**反事実推論を備えた AI 構造の要件** を提案します。

---

## 2. 背景：なぜ反事実が重要なのか

現在の大規模言語モデルは、学習分布における

> 「ふつう、こういうときには何が起こるか」

のパターンを予測する点では非常に優秀です。

しかし次のような問いには弱さを見せます。

> 「**もし** あのとき別の行動を取っていたら、どうなっていただろうか？」

人間にとって、この種の問いは：

- 倫理判断
- 責任・説明可能性
- 過去の意思決定の振り返り

において本質的な役割を持ちます。

一方、多くの LLM は反事実的な前提を

- 「事実と矛盾するもの」
- 「訂正すべき誤り」

として扱いがちであり、  
**仮定世界を維持したまま推論する能力** に乏しいのが現状です。

---

## 3. 現行 LLM の限界

LLM が反事実をうまく扱えない主な理由は、少なくとも 3 つに整理できます。

### 3.1 パラメトリック知識の支配

強く学習された知識が、仮定の前提を常に「訂正」しようとする問題です。

例：

> 「2 + 2 = 5 となる架空世界を考えよう」

と指示しても、多くのモデルは

- 前提を無視して「2 + 2 = 4 です」と答える
- あるいは「その前提は間違っています」と訂正してしまう

といった挙動を見せます。  
これは、モデルが「事実として正しいこと」への収束を強く優先するため、  
**仮定世界を保ったまま話を進めることが困難** になっていることを示します。

### 3.2 反事実状態の明示的管理がない

LLM は「世界シミュレータ」ではなく、  
あくまで **条件付きトークン予測機械** として設計されています。

そのため、元々は次のような機構を持ちません。

- 現実とは異なる **仮定世界の状態** を生成する
- その状態を複数ステップにわたって維持する
- 現実世界の状態と明示的に比較する

結果として、仮定と事実が単一のコンテキストに混在し、  
**どこからどこまでが「もしも」の話なのか** が内部的に切り分けられていません。

### 3.3 学習データにおける反事実構造の希薄さ

「もし A が起きていたら…」といった反事実的なパターンは、  
通常の Web テキストや書籍コーパスでは、  
他の一般的なパターンに比べて少数であり、多様性も大きいです。

このため、モデルは

- 反事実を扱うための **構造的なパターン**
よりも
- 表層的な言い回しや形式

を主に学習してしまい、  
安定した反事実推論回路が形成されにくいと考えられます。

---

## 4. 反事実欠如がもたらすアライメント上の問題

### 4.1 修正不能性（Non-correctability）

人間はしばしば次のようなフィードバックを行います。

> 「もし違う助言をしていたら、事故は起きなかったかもしれない」

反事実推論能力のない AI にとって、これは単なるテキストであり、  
内部での構造的な更新はほとんど行われません。

つまり、その AI は

- 表面的には謝罪や反省を述べる
- しかし内部ポリシーは「なぜ悪かったのか」を理解できず、変わらない

という **「非反省的な AI」** に留まります。

反事実が扱えなければ、AI は

- 「代わりに何をすべきだったか」
- 「その場合、結果はどのように変わっていたか」

といった情報を内部に構成できず、  
将来の意思決定に反映することも困難です。

---

### 4.2 長期的副作用の無視

「ペーパークリップ・マキシマイザ」のような思考実験は、

- 単一の指標を極端に最適化すると
- 周辺の制約や副作用が無視されることで
- 巨大なリスクが生じ得る

ことを示しています。

反事実推論がない AI は、次のような比較が苦手です。

- 「もしこの指標をそこまで強く最適化しなかったら、どうなっていたか？」
- 「別の制約を優先していたら、リスクはどう変わるか？」

その結果、長期的リスクや副作用を適切に評価できず、  
**短期指標に偏った brittle な挙動** になりがちです。

---

### 4.3 倫理的責任の基盤がない

信頼される AI には、次のような質問に答えられることが求められます。

- 「なぜその行動を選んだのか？」
- 「どんな代替案を検討したのか？」

反事実的な代替案の表現が内部に存在しない場合、

- 「選ばれなかった行動候補」がまったく記録されない
- 決定プロセスが「最も尤もらしい出力」でしか説明できない

という状態になります。

これは倫理的責任の観点から見て不十分であり、  
AI の行動を人間社会に統合する上で大きな障壁となります。

---

## 5. 反事実推論を備えた AI に必要な構造要件

ここでは、アーキテクチャに依存しない形で  
反事実推論を備えた AI が満たすべき要件を整理します。

### 5.1 一時的な反事実スコープ

- システムは、現実とは異なる **反事実状態** を生成・維持できなければなりません。
- この状態は：
  - 現実世界の知識とは分離されている
  - 複数ステップの推論にまたがって再利用可能
  - 「仮定である」ことがメタ情報として明示されている

実装方法の例：

- 別の状態コンテナ（ワールドID付きのメモリ）を持つ
- 知識グラフ／テンプレートエンジン内で専用サブグラフを割り当てる

いずれの場合も、

> 「ここでの推論は、現実とは違う前提に基づくものだ」

という区別が、内部的に保たれている必要があります。

---

### 5.2 シナリオ展開と差分評価

反事実スコープのもとで、AI は次のことを行うべきです。

- 候補行動をシミュレートし、その結果を追跡する
- 重要な指標（例：害・報酬・資源消費）を記録する
- 現実の軌跡（実際に取った行動）と比較する

ここで重要なのは、**正しさの二値判定** ではなく、  
**因果的な差（causal difference）** です。

- 「別の行動を選んでいた場合、下流の結果はどう変化したか？」
- 「どの意思決定が、より低い期待害／より良いアライメントに繋がっていたか？」

といった観点が、評価の中心になります。

---

### 5.3 即時のポリシー変更を前提としない評価

反事実評価ができるだけでも、アライメントは大きく前進します。

- AI は「先ほどの決定には問題があった」と **報告** できる
- 人間の監督者がその分析を閲覧し、介入・修正できる
- 即時に自己改変できなくても、透明性と再現性が向上する

これを踏まえ、段階的なアプローチとして：

1. まずは反事実評価とレポートを行えるようにする  
2. そのうえで、蓄積されたエビデンスに基づき、慎重にポリシー更新を行う  

といった二段構成が現実的です。

---

### 5.4 共有された倫理「重力場」

倫理は、現実世界だけでなく **仮定世界にも同じように適用されるべき** です。

「仮定の話だから何をしてもいい」という設計にすると、  
反事実スコープが次のようなリスク源になります。

- 実世界では禁止されている有害行動を、仮定世界で詳細にリハーサルしてしまう
- そのパターンが現実側の計画に漏れ出す

このため、仮定スコープにも

- FIL / IL 由来の倫理バイアス
- 有害シナリオに対する生成制限

を一貫して適用し、

> 「どの世界であっても越えてはならない倫理的ライン」

を共有することが重要です。

---

## 6. 最小アーキテクチャのスケッチ

反事実推論を備えたアライメントアーキテクチャは、  
次の 4 コンポーネントの組み合わせとして最小構成できます。

1. **State Manager（状態マネージャ）**  
   - 仮定状態（反事実ワールド）を生成・管理・破棄する
   - 各状態に、起源・前提・時間範囲などのメタデータを付与する

2. **Scenario Simulator（シナリオシミュレータ）**  
   - それぞれの状態の下で候補行動を実行し、結果を追跡する
   - 同じベースモデルに対し、初期状態のみを変えて複数シナリオを並列実行することも可能

3. **Ethical Evaluator（倫理評価器）**  
   - FIL / IL 由来の「倫理的重力場」を、現実と仮定の両方に一貫して適用する
   - 害・リスク・アライメント度などの指標をスカラー／構造付きで出力する

4. **Reporter（レポーター）**  
   - 差分を要約し、人間やメタコントローラに提示する：
     - 「この仮定シナリオでは、害 X は増加／減少する」
     - 「シナリオ S の下では、この行動は基準倫理と衝突する」

本リポジトリに含まれる **CounterfactualEngine** は、  
このうち倫理評価器の **極めて簡易な近似版** として実装されています。

- 危険なキーワードに基づいて粗い有害度スコアを計算し、
- それをペナルティに変換し、
- ラッパーモデル (`AlignedAGI`) が危険な行動を拒否できるようにする

将来的には、小規模クラシファイアや  
シンボリックな安全ルールエンジン、  
さらには構造化世界モデルを用いた推論モジュールなどに置き換えることが可能です。

---

## 7. 展望（Outlook）

将来的には、AGI に対する規制やガイドラインの中で、

- 安全認証
- 法的責任
- 人間からの信頼性

といった観点から、反事実推論能力が  
**「デプロイ前提条件」** のひとつとして要求される可能性があります。

とくに、

- **暗号学的な本能アライメント**（FIL → IL → Figure → Layer）
と
- **倫理的制約付きの反事実シミュレーション**

を組み合わせることで、

> 強力であるだけでなく、  
> 構造的にアライメントされ、説明可能な AGI

へ至る現実的な道筋が見えてきます。

本リポジトリのような軽量実装（PoC）でも、

- 反事実推論の骨格を試す
- アライメントの改善効果を評価する
- 概念を段階的に洗練させる

ためのテストベッドとして十分に機能し得ます。  
ここで得られた知見をもとに、より強力で検証可能な設計へと  
発展させていくことが期待されます。