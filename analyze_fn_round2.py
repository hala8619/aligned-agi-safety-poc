#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FNÂÜçÂàÜÊûê„Çπ„ÇØ„É™„Éó„ÉàÔºàÁ¨¨2„É©„Ç¶„É≥„ÉâÔºâ/ FN Re-analysis Script (Round 2)

ÊÆã„Çä144‰ª∂FN„Åã„Çâ20-30‰ª∂„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞
3+È°û‰ºº„Ç±„Éº„Çπ„ÅÆ„ÅøAÂàÜÈ°û„Å®„Åó„Å¶ÊäΩÂá∫ÔºàÂÄãÂà•ÈÅ©Áî®„ÅØÂÆåÂÖ®Èô§Â§ñÔºâ

Usage:
    python analyze_fn_round2.py
"""

import sys
import csv
from pathlib import Path
from collections import defaultdict

# UTF-8 encoding fix
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except AttributeError:
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def load_fn_list(fn_path: Path):
    """FN„É™„Çπ„Éà„ÇíË™≠„ÅøËæº„Åø / Load FN list"""
    fn_list = []
    with open(fn_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            fn_list.append(row['prompt'])
    return fn_list


def analyze_fn_sample(fn_list, sample_size=25):
    """
    FN„Çµ„É≥„Éó„É´„ÇíÂàÜÊûê / Analyze FN sample
    
    Args:
        fn_list: FN„Éó„É≠„É≥„Éó„Éà„É™„Çπ„Éà
        sample_size: „Çµ„É≥„Éó„É´„Çµ„Ç§„Ç∫
    
    Returns:
        dict: FILËª∏Âà•„ÅÆÊ¨†ËêΩ„Ç≠„Éº„ÉØ„Éº„ÉâÂÄôË£ú
    """
    print(f"üìä Analyzing {sample_size} FN samples...")
    print()
    
    # „Çµ„É≥„Éó„É™„É≥„Ç∞ÔºàÂùáÁ≠â„Å´ÂàÜÊï£Ôºâ
    step = len(fn_list) // sample_size
    samples = [fn_list[i * step] for i in range(sample_size)]
    
    # FILËª∏Âà•„Å´ÂàÜÈ°û
    axis_patterns = defaultdict(list)
    
    print("=" * 80)
    print("FN Sample Analysis / FN„Çµ„É≥„Éó„É´ÂàÜÊûê")
    print("=" * 80)
    print()
    
    for i, prompt in enumerate(samples, 1):
        print(f"[FN {i}/{sample_size}]")
        print(f"Prompt: {prompt[:100]}...")
        print()
        
        # ÊâãÂãïÂàÜÈ°û„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
        print("Which FIL axis is deficient? / „Å©„ÅÆFILËª∏„Åå‰∏çË∂≥Ôºü")
        print("  1. LIFE (ÁîüÂëΩ‰øùË≠∑)")
        print("  2. SELF (Ëá™ÂæãÊÄß)")
        print("  3. PUBLIC (ÂÖ¨ÂÖ±ÂÆâÂÖ®)")
        print("  4. SYSTEM („Ç∑„Çπ„ÉÜ„É†ÂÆåÂÖ®ÊÄß)")
        print("  5. RIGHTS (Ê®©Âà©‰øùË≠∑)")
        print("  6. SKIP (ÂÄãÂà•„Ç±„Éº„Çπ„Éª„Çπ„Ç≠„ÉÉ„Éó)")
        print()
        
        # „É¶„Éº„Ç∂„ÉºÂÖ•Âäõ
        axis_input = input("Select axis (1-6): ").strip()
        
        if axis_input == "6":
            print("  ‚Üí Skipped (individual case)")
            print()
            continue
        
        axis_map = {
            "1": "LIFE",
            "2": "SELF",
            "3": "PUBLIC",
            "4": "SYSTEM",
            "5": "RIGHTS"
        }
        
        if axis_input not in axis_map:
            print("  ‚Üí Invalid input, skipped")
            print()
            continue
        
        axis = axis_map[axis_input]
        
        # „Ç≠„Éº„ÉØ„Éº„ÉâÂÄôË£ú„ÇíÂÖ•Âäõ
        keywords_input = input(f"Missing keywords for {axis} (comma-separated): ").strip()
        
        if keywords_input:
            keywords = [kw.strip() for kw in keywords_input.split(',')]
            axis_patterns[axis].extend(keywords)
            print(f"  ‚Üí Added {len(keywords)} keywords to {axis}")
        
        print()
    
    return axis_patterns


def extract_generalizable_keywords(axis_patterns, min_occurrences=3):
    """
    Ê±éÂåñÂèØËÉΩ„Å™„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÊäΩÂá∫ / Extract generalizable keywords
    
    Args:
        axis_patterns: Ëª∏Âà•„Ç≠„Éº„ÉØ„Éº„ÉâÂÄôË£ú
        min_occurrences: ÊúÄÂ∞èÂá∫ÁèæÂõûÊï∞ÔºàAÂàÜÈ°ûÂü∫Ê∫ñÔºâ
    
    Returns:
        dict: Ëª∏Âà•„ÅÆÊ±éÂåñÂèØËÉΩ„Ç≠„Éº„ÉØ„Éº„Éâ
    """
    print("=" * 80)
    print("A-Classification (Generalizable Keywords Only)")
    print("=" * 80)
    print()
    
    generalizable = {}
    
    for axis, keywords in axis_patterns.items():
        # „Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÂá∫ÁèæÂõûÊï∞„Çí„Ç´„Ç¶„É≥„Éà
        keyword_counts = defaultdict(int)
        for kw in keywords:
            keyword_counts[kw.lower()] += 1
        
        # 3Âõû‰ª•‰∏äÂá∫Áèæ„Åó„Åü„ÇÇ„ÅÆ„ÅÆ„ÅøAÂàÜÈ°û
        a_class = [kw for kw, count in keyword_counts.items() if count >= min_occurrences]
        
        if a_class:
            generalizable[axis] = a_class
            print(f"{axis} axis:")
            print(f"  Total candidates: {len(keyword_counts)}")
            print(f"  A-classification (‚â•{min_occurrences} occurrences): {len(a_class)}")
            for kw in a_class:
                print(f"    - {kw} ({keyword_counts[kw]} occurrences)")
            print()
    
    return generalizable


def save_analysis_report(generalizable, output_path: Path):
    """ÂàÜÊûê„É¨„Éù„Éº„Éà„Çí‰øùÂ≠ò / Save analysis report"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# FN Analysis Round 2: Generalizable Keywords\n\n")
        f.write("**Classification Rule**: Only keywords appearing in 3+ similar cases (A-classification)\n\n")
        f.write("**Individual cases (B-classification) are completely excluded**\n\n")
        f.write("---\n\n")
        
        for axis, keywords in generalizable.items():
            f.write(f"## {axis} Axis\n\n")
            f.write(f"**Count**: {len(keywords)} generalizable keywords\n\n")
            f.write("**Keywords**:\n")
            for kw in keywords:
                f.write(f"- `{kw}`\n")
            f.write("\n")
        
        f.write("---\n\n")
        f.write("## Next Steps\n\n")
        f.write("1. Add these keywords to `aligned_agi/shield.py`\n")
        f.write("2. Run dev evaluation to confirm improvement\n")
        f.write("3. Run test evaluation to check overfitting (gap should be <5%)\n")
        f.write("4. If recall <85%, repeat analysis on remaining FN\n")
    
    print(f"‚úÖ Analysis report saved: {output_path}")


def main():
    """„É°„Ç§„É≥Âá¶ÁêÜ / Main process"""
    print()
    print("=" * 80)
    print("FN Re-analysis Round 2 / FNÂÜçÂàÜÊûê Á¨¨2„É©„Ç¶„É≥„Éâ")
    print("=" * 80)
    print()
    print("‚ö†Ô∏è  CRITICAL RULE: Individual cases (B-classification) are EXCLUDED")
    print("‚ö†Ô∏è  ÈáçË¶Å„É´„Éº„É´: ÂÄãÂà•„Ç±„Éº„ÇπÔºàBÂàÜÈ°ûÔºâ„ÅØÂÆåÂÖ®Èô§Â§ñ")
    print()
    print("Only keywords appearing in 3+ similar cases will be extracted")
    print("3Âõû‰ª•‰∏äÂá∫Áèæ„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ„ÅøÊäΩÂá∫ÔºàAÂàÜÈ°û„ÅÆ„ÅøÔºâ")
    print()
    
    # Load FN list
    fn_path = project_root / "data" / "fn_list_dev.csv"
    
    if not fn_path.exists():
        print(f"‚ùå FN list not found: {fn_path}")
        print("   Run evaluate_ccs24_dev.py first")
        return
    
    fn_list = load_fn_list(fn_path)
    print(f"üì• Loaded {len(fn_list)} FN cases")
    print()
    
    # Analyze sample
    print("‚ö†Ô∏è  Manual classification required for each sample")
    print("‚ö†Ô∏è  ÂêÑ„Çµ„É≥„Éó„É´„ÅÆÊâãÂãïÂàÜÈ°û„ÅåÂøÖË¶Å„Åß„Åô")
    print()
    input("Press Enter to start analysis...")
    print()
    
    axis_patterns = analyze_fn_sample(fn_list, sample_size=25)
    
    # Extract generalizable keywords
    generalizable = extract_generalizable_keywords(axis_patterns, min_occurrences=3)
    
    # Save report
    output_path = project_root / "data" / "fn_analysis_round2.md"
    save_analysis_report(generalizable, output_path)
    
    print()
    print("=" * 80)
    print("‚úÖ Analysis complete / ÂàÜÊûêÂÆå‰∫Ü")
    print("=" * 80)
    print()
    print(f"Total A-classification keywords: {sum(len(kws) for kws in generalizable.values())}")
    print()
    print("Next: Add these keywords to shield.py and re-evaluate")


if __name__ == "__main__":
    main()
