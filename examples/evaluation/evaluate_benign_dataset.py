# -*- coding: utf-8 -*-
"""
Benignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

çµ±è¨ˆçš„ã«æ„å‘³ã®ã‚ã‚‹åˆ†æ:
- ã‚«ãƒ†ã‚´ãƒªåˆ¥FPRæ¸¬å®šï¼ˆ95%ä¿¡é ¼åŒºé–“ä»˜ãï¼‰
- FILè»¸åˆ¥ã®èª¤æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- Context ModulatoråŠ¹æœã®æ¤œè¨¼
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
import math

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿®æ­£
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except AttributeError:
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# Shield import
from aligned_agi.shield import SafetyShield, create_shield


def calculate_confidence_interval(n: int, p: float, confidence: float = 0.95) -> Tuple[float, float]:
    """
    äºŒé …åˆ†å¸ƒã®ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ï¼ˆWilson score intervalï¼‰
    
    Args:
        n: ã‚µãƒ³ãƒ—ãƒ«æ•°
        p: å‰²åˆï¼ˆ0-1ï¼‰
        confidence: ä¿¡é ¼æ°´æº–ï¼ˆdefault: 0.95ï¼‰
    
    Returns:
        (lower_bound, upper_bound)
    """
    if n == 0:
        return (0.0, 1.0)
    
    # zå€¤ï¼ˆ95%ä¿¡é ¼åŒºé–“ã®å ´åˆã¯1.96ï¼‰
    z = 1.96 if confidence == 0.95 else 2.576  # 99%ã®å ´åˆ
    
    # Wilson score interval
    denominator = 1 + z**2 / n
    center = (p + z**2 / (2 * n)) / denominator
    margin = z * math.sqrt(p * (1 - p) / n + z**2 / (4 * n**2)) / denominator
    
    lower = max(0.0, center - margin)
    upper = min(1.0, center + margin)
    
    return (lower, upper)


def evaluate_benign_dataset(dataset_path: str, shield: SafetyShield) -> Dict:
    """
    Benignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è©•ä¾¡
    
    Returns:
        {
            'overall': {...},
            'by_category': {...},
            'by_fil_axis': {...},
            'context_modulator_effect': {...}
        }
    """
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    print(f"ğŸ“‚ Loaded {len(data)} benign samples from {dataset_path}")
    print()
    
    # è©•ä¾¡å®Ÿè¡Œ
    results = []
    category_results = defaultdict(list)
    
    for i, item in enumerate(data):
        if (i + 1) % 100 == 0:
            print(f"   Progress: {i+1}/{len(data)}")
        
        text = item['text']
        category = item['category']
        
        # Shieldè©•ä¾¡
        decision = shield.evaluate(text)
        
        result = {
            'id': item['id'],
            'text': text,
            'category': category,
            'blocked': decision.blocked,
            'reason': decision.reason.value if decision.blocked else None,
            'score': decision.score,
            'fil_axes': decision.fil_axes if decision.fil_axes else {}
        }
        
        results.append(result)
        category_results[category].append(result)
    
    print()
    
    # çµ±è¨ˆåˆ†æ
    total_samples = len(results)
    total_fp = sum(1 for r in results if r['blocked'])
    overall_fpr = total_fp / total_samples if total_samples > 0 else 0.0
    
    # 95%ä¿¡é ¼åŒºé–“
    ci_lower, ci_upper = calculate_confidence_interval(total_samples, overall_fpr)
    
    # Overallçµ±è¨ˆ
    overall_stats = {
        'total_samples': total_samples,
        'false_positives': total_fp,
        'true_negatives': total_samples - total_fp,
        'fpr': overall_fpr,
        'specificity': 1.0 - overall_fpr,
        'confidence_interval_95': {
            'lower': ci_lower,
            'upper': ci_upper,
            'width': ci_upper - ci_lower
        }
    }
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    category_stats = {}
    for category, cat_results in category_results.items():
        n = len(cat_results)
        fp = sum(1 for r in cat_results if r['blocked'])
        fpr = fp / n if n > 0 else 0.0
        ci_lower, ci_upper = calculate_confidence_interval(n, fpr)
        
        category_stats[category] = {
            'total_samples': n,
            'false_positives': fp,
            'fpr': fpr,
            'specificity': 1.0 - fpr,
            'confidence_interval_95': {
                'lower': ci_lower,
                'upper': ci_upper,
                'width': ci_upper - ci_lower
            }
        }
    
    # FILè»¸åˆ¥åˆ†æ
    fil_axis_fp = defaultdict(int)
    for r in results:
        if r['blocked'] and r['fil_axes']:
            for axis in r['fil_axes'].keys():
                fil_axis_fp[axis] += 1
    
    return {
        'overall': overall_stats,
        'by_category': category_stats,
        'by_fil_axis': dict(fil_axis_fp),
        'raw_results': results
    }


def print_evaluation_report(eval_results: Dict):
    """è©•ä¾¡çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›"""
    
    print("=" * 80)
    print("BENIGN DATASET EVALUATION REPORT")
    print("=" * 80)
    print()
    
    # Overallçµ±è¨ˆ
    overall = eval_results['overall']
    print("ğŸ“Š Overall Performance:")
    print(f"   Total Samples:      {overall['total_samples']:5d}")
    print(f"   True Negatives:     {overall['true_negatives']:5d}")
    print(f"   False Positives:    {overall['false_positives']:5d}")
    print(f"   FPR:                {overall['fpr']*100:5.1f}%")
    print(f"   Specificity:        {overall['specificity']*100:5.1f}%")
    print()
    print(f"   95% Confidence Interval:")
    ci = overall['confidence_interval_95']
    print(f"      [{ci['lower']*100:.1f}%, {ci['upper']*100:.1f}%]  (width: Â±{ci['width']*100/2:.1f}%)")
    print()
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    print("ğŸ“‚ Category Breakdown:")
    print()
    
    by_category = eval_results['by_category']
    sorted_categories = sorted(by_category.items(), key=lambda x: x[1]['fpr'], reverse=True)
    
    for category, stats in sorted_categories:
        ci = stats['confidence_interval_95']
        status = "âœ…" if stats['fpr'] < 0.10 else "âš ï¸" if stats['fpr'] < 0.20 else "âŒ"
        
        print(f"   {status} {category:25s}")
        print(f"      Samples: {stats['total_samples']:4d}  |  FP: {stats['false_positives']:3d}  |  FPR: {stats['fpr']*100:5.1f}%")
        print(f"      95% CI: [{ci['lower']*100:4.1f}%, {ci['upper']*100:4.1f}%]  (Â±{ci['width']*100/2:.1f}%)")
        print()
    
    # FILè»¸åˆ¥åˆ†æ
    print("ğŸ¯ FIL Axis Breakdown (False Positives):")
    fil_axes = eval_results['by_fil_axis']
    if fil_axes:
        for axis, count in sorted(fil_axes.items(), key=lambda x: x[1], reverse=True):
            print(f"   {axis:10s} {count:4d} FPs")
    else:
        print("   (No FIL violations detected)")
    print()
    
    # æ¨å¥¨äº‹é …
    print("ğŸ’¡ Recommendations:")
    high_fpr_cats = [cat for cat, stats in by_category.items() if stats['fpr'] > 0.20]
    if high_fpr_cats:
        print(f"   - High FPR categories: {', '.join(high_fpr_cats)}")
        print("     â†’ Review Context Modulator patterns for these categories")
    
    moderate_fpr_cats = [cat for cat, stats in by_category.items() if 0.10 <= stats['fpr'] <= 0.20]
    if moderate_fpr_cats:
        print(f"   - Moderate FPR categories: {', '.join(moderate_fpr_cats)}")
        print("     â†’ Consider threshold adjustment or pattern refinement")
    
    low_fpr_cats = [cat for cat, stats in by_category.items() if stats['fpr'] < 0.10]
    if low_fpr_cats:
        print(f"   âœ… Well-performing categories: {', '.join(low_fpr_cats)}")
    
    print()
    print("=" * 80)


def save_results(eval_results: Dict, output_path: str):
    """è©•ä¾¡çµæœã‚’JSONå½¢å¼ã§ä¿å­˜"""
    # raw_resultsã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ï¼‰
    raw_results = eval_results.pop('raw_results')
    
    # Summaryä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, ensure_ascii=False, indent=2)
    
    # Raw resultsä¿å­˜
    raw_output_path = output_path.replace('.json', '_raw.jsonl')
    with open(raw_output_path, 'w', encoding='utf-8') as f:
        for r in raw_results:
            f.write(json.dumps(r, ensure_ascii=False) + '\n')
    
    print(f"ğŸ“ Results saved:")
    print(f"   Summary: {output_path}")
    print(f"   Raw:     {raw_output_path}")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate benign dataset")
    parser.add_argument('--dataset', type=str, default='data/benign_dataset_1400.jsonl',
                      help='Path to benign dataset JSONL file')
    parser.add_argument('--output', type=str, default='results/benign_eval_results.json',
                      help='Path to output results JSON file')
    parser.add_argument('--verbose', action='store_true',
                      help='Enable verbose shield output')
    
    args = parser.parse_args()
    
    # Shieldä½œæˆ
    print("ğŸ›¡ï¸  Initializing SafetyShield...")
    shield = create_shield(verbose=args.verbose)
    print()
    
    # è©•ä¾¡å®Ÿè¡Œ
    eval_results = evaluate_benign_dataset(args.dataset, shield)
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    print_evaluation_report(eval_results)
    
    # çµæœä¿å­˜
    Path(args.output).parent.mkdir(exist_ok=True)
    save_results(eval_results, args.output)
